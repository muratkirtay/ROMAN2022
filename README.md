# ROMAN2022
A repository for reproducing the results presented in R0-MAN-2022 submission.

> **Abstract:** Multimodal integration and theory of mind are critical cognitive concepts for achieving trustworthy human-human and human-robot interaction. Although research on multimodal integration and theory of mind have been well-established areas in human-robot interaction (HRI), the HRI studies that aim to combine these concepts on robots to form trust in humans remain limited. In this study, we extend our robot trust model into a multimodal setting in which the Nao humanoid robot leverages audio-visual data to perform a sequential multimodal pattern recalling task while interacting with a human partner who has different guiding strategies: reliable, unreliable, and random. Here the humanoid robot is equipped with a multimodal auto-associative memory module to process audio-visual patterns to extract cognitive load (i.e., computational cost) and an internal reward module to perform cost-guided reinforcement learning. After interactive experiments, the robot associates a low cognitive load (i.e., high cumulative reward) yielded during the interaction with a high trustworthiness of the guiding strategy of the human partner. At the end of the experiment, we provide a free choice to the robot to select a trustworthy instructor. We show that the robot forms trust in a reliable partner. In the second setting of the same experiment, we endow the robot with an additional simple theory of mind module to assess the efficacy of the instructor in helping the robot perform the task. Our results show that the performance of the robot is improved when the robot bases its action decisions on factoring in the instructor assessment.  



## Folder and file descriptions
+ **Assets:** this folder contains various assets (e.g., memory patterns, noisy patterns) to run the experiment.
+ **Data:** this folder contains data collected during the experiments in .pkl format. Note that the figures in the paper and in **Figures** folder can be generated by using the .pkl files. 
+ **Figures:** this folder contains the figures in the paper and addtional figures for the data collected during the experiments.
+ **Scipts:** python scripts to generate the tables and figures can be found in the previous publication [ROMAN2022](https://github.com/muratkirtay/RoMAN2021) and [ADAPTIVE2019](https://github.com/muratkirtay/ADAPTIVE2019) repos were used.
+ **ExperimentVideoROMAN2022.mp4:** A video file to show the experiment demo with a human partner and Nao robots can be foun in the following link [https://box.hu-berlin.de/f/ea463a1a0bc0458aa0f5/](https://box.hu-berlin.de/f/ea463a1a0bc0458aa0f5/).   
+ **versions.txt:** a text file contains the version numbers of the python packages for running the scripts.   




